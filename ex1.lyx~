#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\begin_modules
enumitem
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 0.2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
APML - Ex 1
\end_layout

\begin_layout Author
Rhea Chowers, 204150643
\end_layout

\begin_layout Section
Theoretical Questions
\end_layout

\begin_layout Enumerate
The function defined will not define a larger hypothesis class when incorperated
 into a network.
 Assume the parametrized relu comes after some linear layer.
 We'll show that the bias term of the linear layer controls the relu's parameter
, making it unnecessary: let 
\begin_inset Formula $t=Wx+b$
\end_inset

 since we're after a linear layer.
 Thus 
\begin_inset Formula 
\[
f_{i}(o;t)=\max{t,o_{i}}=\max{Wx+b,o_{i}}=\max{Wx+b-o_{i},0}=\max{Wx+b',0}
\]

\end_inset

 for 
\begin_inset Formula $b'=b-o_{i}$
\end_inset

.
 The linear layer can learn the parameter b' instead of b, making the combinatio
n of the linear layer followed by a relu as expressive as a linear layer
 followed by a parameterized relu.
 Thus parameterizing the relu function increases allows the network to define
 a larger hypothesis class only when there are no layers with added bias
 before it.
\end_layout

\begin_layout Enumerate
Derivative of the function:
\begin_inset Formula 
\[
\frac{d}{dx}\sigma\left(x\right)=-\left(\frac{d}{dx}\left(1+e^{-x}\right)\right)\cdot\left(\frac{1}{1+e^{-x}}\right)^{2}=e^{-x}\left(\frac{1}{1+e^{-x}}\right)^{2}=\sigma\left(x\right)\cdot\frac{e^{-x}}{1+e^{-x}}=\sigma\left(x\right)\cdot\left(\frac{1+e^{-x}-1}{1+e^{-x}}\right)=\sigma\left(x\right)\cdot\left(1-\sigma\left(x\right)\right)
\]

\end_inset

The computational graph:
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename sigmoid.png
	scale 50

\end_inset


\end_layout

\begin_layout Section
Linear Regression
\end_layout

\begin_layout Standard
In the following graph we can see the loss at the end of each epoch for
 a learning rate of lr=0.0001.
 This creates a smooth curve.
 In the following graph we can see what happens with a significantly higher
 learning rate of 0.01:
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename Linear Regression Loss with Learning Rate = 0_0001.png

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename Linear Regression Loss with Learning Rate = 0_01.png

\end_inset


\end_layout

\begin_layout Section
MLP and ConvNet
\end_layout

\begin_layout Standard
When training the MLP without regularization, we can see that after an initial
 significant drop in the test loss, we begin to overfit - the training loss
 continues to decrease but the test loss increases significantly.
 Thus we can identify a 
\begin_inset Quotes eld
\end_inset

stage
\begin_inset Quotes erd
\end_inset

 in learning process of the beginning of overfitting at around 15 epochs.
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename mlp.png

\end_inset


\begin_inset Newline newline
\end_inset

After applying weight regularization to the network, we can see that overfitting
 is prevented and the test loss and accuracy circle a constant low number:
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename regularized mlp.png

\end_inset


\end_layout

\begin_layout Section
Adversarial Image
\end_layout

\begin_layout Standard
In order to create an adversarial image, I took the trained model and fed
 it images that it classifies correctly.
 After getting the loss, I calculated its gradient with 
\series bold
respect to the input 
\series default
(and not the network weights) and then updated the images using the fast
 gradient sign method.
 Since the MLP is composed of linear and activational (relu) layers, updating
 the image to be 
\begin_inset Formula $image=image+\epsilon\cdot sign\left(\frac{dLoss}{dImage}\right)$
\end_inset

 we will 'overactivate' some of the inner layers and destroy the prediction.
 For example:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename adversarial 22 epsilon = 0_01.png

\end_inset


\end_layout

\end_body
\end_document
